#!/usr/bin/env python
# coding: utf-8

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
import re
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords, wordnet

# Step 1: Setup & downloads of NLTK 
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('averaged_perceptron_tagger')

# Step 2: Cleaning & Preprocessing Text
# Initializing the lemmatizer
lemmatizer = WordNetLemmatizer()

# Getting wordnet POS (part of speech) for lemmatization
def get_wordnet_pos(word):
    """Map POS tag to first character lemmatize() accepts"""
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ,
                "N": wordnet.NOUN,
                "V": wordnet.VERB,
                "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)  # Wang, A. P. (2020, September 29). Can you tell someone’s gender based on tweets? – AI Journey.
                                                # https://ai-journey.com/2020/09/can-you-tell-someones-gender-based-on-tweets/

def preprocess_text(text, language='english'):
    text = re.sub(r'[^\w\s]', '', text)  # punctuation
    text = re.sub(r'\d+', '', text)  # digits
    text = text.lower()  # lowercasing  # Konjeti, V. a. P. B. C. (2020b, June 4). Text preprocessing techniques for NLP tasks. Chaitu Konjeti.
                                            # https://chaitukonjeti.me/2020/06/04/text-preprocessing-techniques-for-nlp-tasks/

    # Stopwords based on language (German/English)
    stopwords_set = stopwords.words('german') if language == 'german' else stopwords.words('english')
    
    # Tokenizing & Stopword Removal
    words = text.split()
    words = [word for word in words if word not in stopwords_set]

    # Lemmatization
    words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]

    return ' '.join(words)

# Step 3: Loading & Preparing Data
file_path = '/Users/tania_ren/Downloads/Multilingual_Dataset.csv'
data = pd.read_csv(file_path, delimiter=';', quotechar='"', engine='python')

# Combining text columns for feature extraction (the "name" of the ticket & its "body")
data['text_combined'] = data['subject'] + " " + data['text']
data['cleaned_text_combined'] = data['text_combined'].apply(preprocess_text)

# Converting category_number to integers
data['category_number'] = data['category_number'].astype(int)

# Step 4: Feature Extraction
vectorizer = TfidfVectorizer(max_features=10000)
X = vectorizer.fit_transform(data['cleaned_text_combined'])
y = data['category_number']

# Step 5: Training the Model (70% training, 20% validation, 10% test)
train_texts, temp_texts, train_labels, temp_labels = train_test_split(texts, labels, test_size=0.3, random_state=42)
val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.33, random_state=42) #Data-Driven Science. (2023, July 5). 
                                                                                                                            #Building text classification models using BERT. Data-Driven Science. https://datadrivenscience.com/building-text-classification-models-using-bert/



# Step 6: Evaluation Metrics (Accuracy, Precision, Recall, F1)
test_preds = lr_model.predict(test_vectors)
test_accuracy = accuracy_score(test_labels, test_preds)
test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(test_labels, test_preds, average='weighted')

print(f'Test Accuracy: {test_accuracy}')
print(f'Test Precision: {test_precision}')
print(f'Test Recall: {test_recall}')
print(f'Test F1 Score: {test_f1}')


# Step 7: Confusion Matrix visualization
min_length = min(len(test_labels), len(test_preds))
trimmed_test_labels = test_labels[:min_length]
trimmed_test_preds = test_preds[:min_length]

test_conf_matrix = confusion_matrix(trimmed_test_labels, trimmed_test_preds)
print(f'Confusion Matrix:\n{test_conf_matrix}')

# Class labels
class_labels = ['class 1', 'class 2', 'class 3']

plt.figure(figsize=(10, 8))
sns.heatmap(test_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Test Set Confusion Matrix')
plt.show()
