#!/usr/bin/env python
# coding: utf-8

# The following code was inspired by HuggingFace. #KameronB/SITCC-Incident-Request-Classifier · Hugging face. (n.d.). 
                                                        #https://huggingface.co/KameronB/SITCC-Incident-Request-Classifier

# #  Data Exploration # # (Data exploration, visualizations, sentiment analysis)  and category prediction on multilingual data using DistilBERT.

# Step 1: Imports
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from textblob import TextBlob
import torch
import re
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset
from tqdm import tqdm
from transformers import get_linear_schedule_with_warmup

from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification

# Step 2: Data loading
file_path = '/Users/tania_ren/Downloads/Multilingual_Dataset.csv'
data = pd.read_csv(file_path, delimiter=';', quotechar='"', engine='python')

# Step 3: Data Preprocessing and observation
# Converting 'category_number' from float to int.
data['category_number'] = data['category_number'].astype(int)

# Cheking for unique labels
unique_labels = data['category_number'].unique()
sorted_unique_labels = sorted(unique_labels)

print(sorted_unique_labels)

# Checking for missing values (No missing values)
missing_values = data.isna().sum()
print("Missing values in each column:")
print(missing_values)

# Step 4: Data Exploration
# First few rows of the dataset
data.head()

# Plotting Distribution of Tickets
# Setting the aesthetic style (blue palette)
sns.set(style="whitegrid")
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 6))
custom_palette = ['#0000FF', '#00008B', '#ADD8E6']  # Blue, Dark Blue & Light Blue

# Distribution of tickets across queues
sns.countplot(data=data, x='queue', ax=axes[0], palette=custom_palette)
axes[0].set_title('Distribution of Tickets by Queue')
axes[0].set_xlabel('Queue')
axes[0].set_ylabel('Number of Tickets')
axes[0].tick_params(axis='x', rotation=45)

# Distribution of ticket priorities 
sns.countplot(data=data, x='priority', ax=axes[1], palette=custom_palette)
axes[1].set_title('Distribution of Tickets by Priority')
axes[1].set_xlabel('Priority')
axes[1].set_ylabel('Number of Tickets')
plt.tight_layout()
plt.show()


# Step 5: Sentiment Analysis
#Calculating sentiment
def get_sentiment(text):
    # Create a TextBlob object
    blob = TextBlob(text)
    # Get sentiment polarity
    polarity = blob.sentiment.polarity
    # Categorize sentiment
    if polarity > 0:
        return 'Positive'
    elif polarity == 0:
        return 'Neutral'
    else:
        return 'Negative'

#Sentiment function to the text column
data['sentiment'] = data['text'].apply(get_sentiment)

#Distribution of sentiments
sentiment_counts = data['sentiment'].value_counts()
sentiment_counts, sentiment_counts.plot(kind='bar', color=['blue', 'lightblue', 'red'], title='Sentiment Distribution')

#Bar chart to visualize relationships between ticket priority levels and sentiments
priority_sentiment_counts = data.groupby(['priority', 'sentiment']).size().unstack().fillna(0)

priority_sentiment_counts.plot(kind='bar', figsize=(12, 6), stacked=True, color=['red', 'blue', 'green'])
plt.title('Sentiment Distribution by Priority')
plt.xlabel('Priority')
plt.ylabel('Number of Tickets')
plt.legend(title='Sentiment')
plt.show()


# #  Distilbert Model # # (Category prediction on multilingual data using DistilBERT)

# Step 1: Further Data Preprocessing 

# Combining subject and text columns - for mode accurate data
data['text_combined'] = data['subject'] + " " + data['text']

# Detecting language of each text, classifying as 'german' or 'english'
data['language'] = data['text_combined'].apply(
    lambda x: 'german' if re.search(r'\b(?:der|die|das|und|ist)\b', x.lower()) else 'english'
)

# Displaying first few rows
data[['text_combined', 'language']].head()

#Displaying the bottom of the dataframe
data[['text_combined', 'language']].tail()

# Step 2: Data Preparation for Model Training
# Applying minimal text preprocessing with lambda
data['cleaned_text_combined'] = data['text_combined'].apply(lambda x: " ".join(x.lower().strip().split()))

# Tokenizing the cleaned text
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')
data['tokens'] = data['cleaned_text_combined'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))

# Pad tokens to ensure a uniform sequence length
max_len = max(data['tokens'].apply(len))
data['padded_tokens'] = data['tokens'].apply(lambda x: x + [0] * (max_len - len(x)))
data['attention_masks'] = data['tokens'].apply(lambda x: [1] * len(x) + [0] * (max_len - len(x)))

# Preparing inputs & labels for our model
input_ids = list(data['padded_tokens'].values)
attention_masks = list(data['attention_masks'].values)
labels = list(data['category_number'].values)

# Initial Checking unique labels
unique_labels = data['category_number'].unique()
num_labels = len(unique_labels)
print(f'Initial unique labels: {unique_labels}')
print(f'Initial number of unique labels: {num_labels}')

# Remapping Labels to a continuous range starting from 0
label_mapping = {label: idx for idx, label in enumerate(sorted(unique_labels))}
data['category_number'] = data['category_number'].map(label_mapping)

# Verifying Remapped Labels
remapped_labels = data['category_number'].unique()
num_labels = len(remapped_labels)
print(f'Remapped labels: {remapped_labels}')
print(f'Number of remapped labels: {num_labels}')

# Updating Labels after remapping
labels = list(data['category_number'].values)

# Checking that all label values are within the correct range
assert all(label in range(num_labels) for label in labels), "Some labels are out of range"

# Step 3: Preparing for Model Training
# Loading pre-trained multilingual DistilBERT model for sequence classification 
model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-multilingual-cased', num_labels=3)

# Preparing inputs & labels
input_ids = list(data['padded_tokens'].values)
attention_masks = list(data['attention_masks'].values)
labels = list(data['category_number'].values)

# Splitting data into training, validation, and test sets (70% training, 20% validation, 10% test)
train_inputs, temp_inputs, train_labels, temp_labels = train_test_split(input_ids, labels, test_size=0.3, random_state=42)
train_masks, temp_masks, _, _ = train_test_split(attention_masks, labels, test_size=0.3, random_state=42)

val_inputs, test_inputs, val_labels, test_labels = train_test_split(temp_inputs, temp_labels, test_size=0.33, random_state=42)
val_masks, test_masks, _, _ = train_test_split(temp_masks, temp_labels, test_size=0.33, random_state=42)

# Converting Inputs to Torch Tensors
train_inputs = torch.tensor(train_inputs)
train_masks = torch.tensor(train_masks)
train_labels = torch.tensor(train_labels)

# Converting Lists to Torch Tensors
val_inputs = torch.tensor(val_inputs)
val_masks = torch.tensor(val_masks)
val_labels = torch.tensor(val_labels)

# Creating DataLoader
batch_size = 8

train_data = TensorDataset(train_inputs, train_masks, train_labels)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)  #Hachcham, A. (2023, August 11). Natural Language Processing with Hugging Face and Transformers. 
                                                                                                #neptune.ai. https://neptune.ai/blog/natural-language-processing-with-hugging-face-and-transformers

val_data = TensorDataset(val_inputs, val_masks, val_labels)
val_sampler = SequentialSampler(val_data)
val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)

# Step 4: Model Training 

# Learning Rate Scheduler Configuration
epochs = 3
total_steps = len(train_dataloader) * epochs

scheduler = get_linear_schedule_with_warmup(optimizer, 
                                            num_warmup_steps=0, 
                                            num_training_steps=total_steps)

# Setting up optimizer with learning rate (0 warmup steps)
optimizer = AdamW(model.parameters(), lr=2e-5)
total_steps = len(train_dataloader) * epochs
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps) #QikaiXu. (n.d.). Writer-Style-Recognition/bert_train.py at main · QikaiXu/Writer-Style-Recognition. GitHub. 
                                                                                                                #https://github.com/QikaiXu/Writer-Style-Recognition/blob/main/bert_train.py

# Initializing variables for predictions and labels
val_preds, val_labels_list = [], []
test_preds, test_labels_list = [], []

# Training loop with progress bars
for epoch in range(epochs):
    print(f'Epoch {epoch+1}/{epochs}')
    print('-' * 10)

    # Training
    model.train()
    total_loss = 0
    train_iterator = tqdm(train_dataloader, desc="Training")
    for step, batch in enumerate(train_iterator):
        batch_input_ids, batch_input_mask, batch_labels = batch
        
        model.zero_grad()
        
        outputs = model(batch_input_ids, 
                        attention_mask=batch_input_mask, 
                        labels=batch_labels)
        
        loss = outputs.loss
        total_loss += loss.item()
        
        loss.backward()
        optimizer.step()
        scheduler.step()
        
        train_iterator.set_postfix({"loss": loss.item()})
        
    avg_train_loss = total_loss / len(train_dataloader)
    print(f'Training loss: {avg_train_loss}')

    # Validation
    model.eval()
    eval_loss = 0
    eval_accuracy = 0
    val_preds_epoch, val_labels_epoch = [], []
    val_iterator = tqdm(val_dataloader, desc="Validation")
    for batch in val_iterator:
        batch_input_ids, batch_input_mask, batch_labels = batch
        
        with torch.no_grad():
            outputs = model(batch_input_ids, 
                            attention_mask=batch_input_mask)
        
        logits = outputs.logits
        predictions = torch.argmax(logits, dim=-1)
        accuracy = (predictions == batch_labels).cpu().numpy().mean() * 100
        eval_accuracy += accuracy
        val_preds_epoch.extend(predictions.cpu().numpy())
        val_labels_epoch.extend(batch_labels.cpu().numpy())  #(KameronB/SITCC-Incident-Request-Classifier · Hugging Face, n.d.)
        
        val_iterator.set_postfix({"accuracy": accuracy})
    
    # Storing predictions and labels for the current epoch
    val_preds.extend(val_preds_epoch)
    val_labels_list.extend(val_labels_epoch)

    avg_val_accuracy = eval_accuracy / len(val_dataloader)
    print(f'Validation Accuracy: {avg_val_accuracy}')
    
    # Calculating metrics
val_preds = np.array(val_preds)
val_labels_list = np.array(val_labels_list)

accuracy = accuracy_score(val_labels_list, val_preds)
precision, recall, f1, _ = precision_recall_fscore_support(val_labels_list, val_preds, average='weighted')

print(f'Accuracy: {accuracy}')
print(f'Precision: {precision}')
print(f'Recall: {recall}')
print(f'F1 Score: {f1}')

# Step 5: Text Classification Function (mapping the output to a human-readable label)

# Creating a Dictionary (mapping category numbers to category label)
category_names = {
    1: "Software",
    2: "Hardware",
    3: "Accounting",
}

def predict_category(text, model, tokenizer, label_mapping, category_names):
    # Preprocessing 
    cleaned_text = preprocess_text(text)
    
    # Tokenizing 
    inputs = tokenizer.encode_plus(
        cleaned_text,
        add_special_tokens=True,
        max_length=max_len,
        pad_to_max_length=True,
        return_tensors="pt",
        return_attention_mask=True
    )
    
    # Input IDs and attention mask tensors
    input_ids = inputs["input_ids"]
    attention_mask = inputs["attention_mask"]
    
    # Making perediction
    model.eval()  
    with torch.no_grad():
        outputs = model(input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        prediction = torch.argmax(logits, dim=-1).item()
    
    # Prediction of index number and label
    inverse_label_mapping = {v: k for k, v in label_mapping.items()}
    predicted_category_number = inverse_label_mapping[prediction]
    predicted_category_name = category_names[predicted_category_number]
    
    return predicted_category_number, predicted_category_name

# Step 6: Model Visuals and Observation

# Converting inputs to torch tensors
test_inputs = torch.tensor(test_inputs)
test_masks = torch.tensor(test_masks)
test_labels = torch.tensor(test_labels)

# Defining the test DataLoader
test_data = TensorDataset(test_inputs, test_masks, test_labels)
test_sampler = SequentialSampler(test_data)
test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)  #Prediction function in an NLP classification problem without using MLM. (2022, May 2). 
                                                                                            #PyTorch Forums. https://discuss.pytorch.org/t/prediction-function-in-an-nlp-classification-problem-without-using-mlm/150596

# Creating Confusion Matrix on Test Set
print("\nRunning on Test Set...")

model.eval()
test_preds, test_labels_list = [], []
test_iterator = tqdm(test_dataloader, desc="Testing")

for batch in test_iterator:
    batch_input_ids, batch_input_mask, batch_labels = batch

    with torch.no_grad():
        outputs = model(batch_input_ids, attention_mask=batch_input_mask)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    test_preds.extend(predictions.cpu().numpy())
    test_labels_list.extend(batch_labels.cpu().numpy())

# Calculating metrics for test set
test_preds = np.array(test_preds)
test_labels_list = np.array(test_labels_list)

test_accuracy = accuracy_score(test_labels_list, test_preds)
test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(test_labels_list, test_preds, average='weighted')

print(f'Test Accuracy: {test_accuracy}')
print(f'Test Precision: {test_precision}')
print(f'Test Recall: {test_recall}')
print(f'Test F1 Score: {test_f1}')

# Confusion Matrix Calculation
test_conf_matrix = confusion_matrix(test_labels_list, test_preds)
print(f'Confusion Matrix:\n{test_conf_matrix}')

# Custom class labels
class_labels = ['class 1', 'class 2', 'class 3']

# Visualizing
plt.figure(figsize=(10, 8))
sns.heatmap(test_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Test Set Confusion Matrix')
plt.show()

# Report for additional metrics
print("\nClassification Report:")
print(classification_report(val_labels_list, val_preds, target_names=[str(label) for label in unique_labels]))

# Step 6: Testing & Saving
# Test 
text_input = "Sehr geehrte Buchhaltung,ich möchte anfragen, ob es möglich ist, auf meiner nächsten Rechnung einen anderen Absendernamen zu verwenden.Vielen Dank!Heike Sander."
predicted_category_number, predicted_category_name = predict_category(text_input, model, tokenizer, label_mapping, category_names)
print(f"Predicted Category Number: {predicted_category_number}")
print(f"Predicted Category Name: {predicted_category_name}")

#Saving model and tokenizer
model_path = "/Users/tania_ren/Downloads/new_distilbert-category-multilingual-classifier-model"
tokenizer_path = "/Users/tania_ren/Downloads/new_distilbert-category-multilingual-tokenizer"
model.save_pretrained(model_path)
tokenizer.save_pretrained(tokenizer_path)

import os
print(os.getcwd())

torch.save(model.state_dict(), "/Users/tania_ren/Downloads/new_distilbert-category-multilingual-weigths/model_weights.pth")
